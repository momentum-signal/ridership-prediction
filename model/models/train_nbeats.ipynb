{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# N-BEATS Model Training Notebook\n",
    "This notebook implements the N-BEATS architecture for ridership prediction\n",
    "\n",
    "\n",
    "### 1. Import Required Libraries"
   ],
   "id": "9d5998b599ac112"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T04:56:49.648284Z",
     "start_time": "2025-05-20T04:56:49.636956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from typing import Tuple, Optional\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import os"
   ],
   "id": "1ef01e13db6829d2",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. Custom module imports",
   "id": "e07bba6d79fad07f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T05:21:11.077223Z",
     "start_time": "2025-05-20T05:21:10.989951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from model.models.train_nbeats import NBeatsBlock, GenericBasis"
   ],
   "id": "c367eab7b14fbacc",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Custom Data Loading and Feature Engineering Modules",
   "id": "64e5a6dc2fa2c830"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T04:42:33.925402Z",
     "start_time": "2025-05-20T04:42:33.891503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RidershipDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for ridership data\"\"\"\n",
    "    def __init__(self, features: np.ndarray, targets: np.ndarray):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: Input features array\n",
    "            targets: Target ridership values\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return (\n",
    "            torch.tensor(self.features[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load data from CSV and create datetime column\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
    "    return df.sort_values(['origin', 'destination', 'datetime'])\n",
    "\n",
    "def add_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add time-based features to dataframe\"\"\"\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['datetime'].dt.hour / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['datetime'].dt.hour / 24)\n",
    "    df['day_of_week'] = df['datetime'].dt.weekday\n",
    "    df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "    # Add lag features (1 week)\n",
    "    df['lag_1_week'] = df.groupby(['origin', 'destination'])['ridership'].shift(24 * 7)\n",
    "    df['lag_1_week'].fillna(0, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def prepare_data(df: pd.DataFrame, test_size: float = 0.2) -> Tuple[DataLoader, DataLoader, MinMaxScaler]:\n",
    "    \"\"\"Prepare data loaders and feature scaler\"\"\"\n",
    "    # Ensure no NaN values\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Separate features and target\n",
    "    feature_cols = ['hour_sin', 'hour_cos', 'lag_1_week', 'day_of_week', 'is_weekend']\n",
    "    features = df[feature_cols].values\n",
    "    target = df['ridership'].values.reshape(-1, 1)\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = MinMaxScaler()\n",
    "    features = scaler.fit_transform(features)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        features, target, test_size=test_size, random_state=42\n",
    "    )\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = RidershipDataset(X_train, y_train)\n",
    "    val_dataset = RidershipDataset(X_val, y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, scaler"
   ],
   "id": "c7ef47755c3d312a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Define the Complete N-BEATS Model",
   "id": "6a0c605b0907fac7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T04:42:46.301814Z",
     "start_time": "2025-05-20T04:42:46.294236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NBeats(pl.LightningModule):\n",
    "    \"\"\"Complete N-BEATS model implemented as PyTorch Lightning module\"\"\"\n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 output_size: int = 1,\n",
    "                 n_stacks: int = 30,\n",
    "                 n_layers: int = 4,\n",
    "                 layer_width: int = 512,\n",
    "                 learning_rate: float = 1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.stacks = nn.ModuleList()\n",
    "        for _ in range(n_stacks):\n",
    "            block = NBeatsBlock(\n",
    "                input_size=input_size,\n",
    "                theta_size=input_size + output_size,\n",
    "                basis_function=GenericBasis(input_size, output_size),\n",
    "                n_layers=n_layers,\n",
    "                layer_width=layer_width\n",
    "            )\n",
    "            self.stacks.append(block)\n",
    "\n",
    "        self.final_layer = nn.Linear(n_stacks * output_size, output_size)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        forecasts = []\n",
    "        for stack in self.stacks:\n",
    "            backcast, forecast = stack(x)\n",
    "            forecasts.append(forecast)\n",
    "            x = x - backcast\n",
    "        forecast = torch.cat(forecasts, dim=1)\n",
    "        return self.final_layer(forecast)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.learning_rate)"
   ],
   "id": "ab9e7c83580463f3",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Training Function",
   "id": "5af49ae6f34432f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T04:52:05.994624Z",
     "start_time": "2025-05-20T04:52:05.987772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(data_path=\"data/cleaned_data.csv\",\n",
    "                n_stacks: int = 30,\n",
    "                n_layers: int = 4,\n",
    "                layer_width: int = 512,\n",
    "                learning_rate: float = 1e-3,\n",
    "                max_epochs: int = 25,\n",
    "                patience: int = 25) -> Tuple[NBeats, MinMaxScaler]:\n",
    "    \"\"\"Complete training pipeline\"\"\"\n",
    "    # Load and prepare data\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    df = load_data(data_path)\n",
    "    df = add_features(df)\n",
    "\n",
    "    # Visualize sample data\n",
    "    print(\"\\nSample training data:\")\n",
    "    display(df.head())\n",
    "\n",
    "    train_loader, val_loader, scaler = prepare_data(df)\n",
    "\n",
    "    # Initialize model\n",
    "    input_size = train_loader.dataset[0][0].shape[0]\n",
    "    print(f\"\\nInitializing model with input size: {input_size}\")\n",
    "\n",
    "    model = NBeats(\n",
    "        input_size=input_size,\n",
    "        n_stacks=n_stacks,\n",
    "        n_layers=n_layers,\n",
    "        layer_width=layer_width,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "\n",
    "    # Configure trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor=\"val_loss\", patience=patience, mode=\"min\"),\n",
    "            ModelCheckpoint(\n",
    "                monitor=\"val_loss\",\n",
    "                dirpath=\"../saved_models/\",\n",
    "                filename=\"nbeats-best\",\n",
    "                save_top_k=1\n",
    "            )\n",
    "        ],\n",
    "        logger=TensorBoardLogger(\"lightning_logs\", name=\"nbeats\"),\n",
    "        accelerator=\"auto\",\n",
    "        devices=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    print(\"\\nStarting training...\")\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    return model, scaler"
   ],
   "id": "a8959d507f14efca",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5. Execute Training",
   "id": "543e9a95baafb571"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T04:59:17.319352Z",
     "start_time": "2025-05-20T04:58:55.202937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = {\n",
    "        \"data_path\": \"../data/cleaned_data.csv\",\n",
    "        \"n_stacks\": 30,\n",
    "        \"n_layers\": 4,\n",
    "        \"layer_width\": 512,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"max_epochs\": 50,\n",
    "        \"patience\": 10,\n",
    "        \"enable_logging\": False  # Set to False to skip TensorBoard\n",
    "    }\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(\"../saved_models\", exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # Handle TensorBoard dependencies\n",
    "        if config[\"enable_logging\"]:\n",
    "            try:\n",
    "                import tensorboard\n",
    "                from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "                logger = TensorBoardLogger(\"lightning_logs\", name=\"nbeats\")\n",
    "                print(\"‚úÖ TensorBoard logging enabled\")\n",
    "            except ImportError:\n",
    "                print(\"‚ö†Ô∏è TensorBoard not available - install with:\")\n",
    "                print(\"!pip install tensorboard tensorboardX\")\n",
    "                config[\"enable_logging\"] = False\n",
    "\n",
    "        # Fix pandas warning\n",
    "        pd.options.mode.chained_assignment = None  # Suppress the warning\n",
    "\n",
    "        # Load and prepare data\n",
    "        print(\"\\nüîç Loading and preprocessing data...\")\n",
    "        df = load_data(config[\"data_path\"])\n",
    "        df = add_features(df)\n",
    "\n",
    "        # Show sample data without copy warnings\n",
    "        with pd.option_context('mode.chained_assignment', None):\n",
    "            print(\"\\nüìä Sample training data:\")\n",
    "            display(df.head())\n",
    "\n",
    "        # Prepare data loaders\n",
    "        train_loader, val_loader, scaler = prepare_data(df)\n",
    "        print(f\"\\nüß† Initializing model with input size: {train_loader.dataset[0][0].shape[0]}\")\n",
    "\n",
    "        # Initialize model\n",
    "        model = NBeats(\n",
    "            input_size=train_loader.dataset[0][0].shape[0],\n",
    "            n_stacks=config[\"n_stacks\"],\n",
    "            n_layers=config[\"n_layers\"],\n",
    "            layer_width=config[\"layer_width\"],\n",
    "            learning_rate=config[\"learning_rate\"]\n",
    "        )\n",
    "\n",
    "        # Configure trainer\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=config[\"max_epochs\"],\n",
    "            callbacks=[\n",
    "                EarlyStopping(monitor=\"val_loss\", patience=config[\"patience\"], mode=\"min\"),\n",
    "                ModelCheckpoint(\n",
    "                    monitor=\"val_loss\",\n",
    "                    dirpath=\"../saved_models/\",\n",
    "                    filename=\"nbeats-best\",\n",
    "                    save_top_k=1\n",
    "                )\n",
    "            ],\n",
    "            logger=logger if config[\"enable_logging\"] else False,\n",
    "            enable_progress_bar=True,\n",
    "            accelerator=\"auto\",\n",
    "            devices=\"auto\"\n",
    "        )\n",
    "\n",
    "        # Train model\n",
    "        print(\"\\nüöÄ Starting training...\")\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "        # Save additional artifacts\n",
    "        torch.save(model.state_dict(), \"../saved_models/nbeats-weights.pt\")\n",
    "        torch.save(scaler, \"../saved_models/nbeats-scaler.pt\")\n",
    "\n",
    "        print(\"\\n‚úÖ Training completed successfully!\")\n",
    "        print(\"Saved artifacts:\")\n",
    "        print(f\"  ‚Ä¢ Model weights: ../saved_models/nbeats-weights.pt\")\n",
    "        print(f\"  ‚Ä¢ Feature scaler: ../saved_models/nbeats-scaler.pt\")\n",
    "        print(f\"  ‚Ä¢ Full checkpoint: ../saved_models/nbeats-best.ckpt\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\n‚ùå Training failed!\")\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        raise e"
   ],
   "id": "10826cf38b460613",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Loading and preprocessing data...\n",
      "\n",
      "üìä Sample training data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2r/x4j4hn691w7gs58r0bq9dtmm0000gn/T/ipykernel_4968/2070104063.py:36: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['lag_1_week'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "             date   time          origin     destination  ridership  \\\n",
       "21554  2025-01-03  22:00  Abdullah Hukum  Abdullah Hukum          2   \n",
       "30734  2025-01-05  14:00  Abdullah Hukum  Abdullah Hukum          1   \n",
       "40019  2025-01-06  19:00  Abdullah Hukum  Abdullah Hukum          2   \n",
       "45529  2025-01-07  16:00  Abdullah Hukum  Abdullah Hukum          1   \n",
       "46881  2025-01-07  19:00  Abdullah Hukum  Abdullah Hukum          1   \n",
       "\n",
       "       day_of_week  is_weekend  is_holiday            datetime  hour_sin  \\\n",
       "21554            4           0           0 2025-01-03 22:00:00 -0.500000   \n",
       "30734            6           1           0 2025-01-05 14:00:00 -0.500000   \n",
       "40019            0           0           0 2025-01-06 19:00:00 -0.965926   \n",
       "45529            1           0           0 2025-01-07 16:00:00 -0.866025   \n",
       "46881            1           0           0 2025-01-07 19:00:00 -0.965926   \n",
       "\n",
       "       hour_cos  lag_1_week  \n",
       "21554  0.866025         0.0  \n",
       "30734 -0.866025         0.0  \n",
       "40019  0.258819         0.0  \n",
       "45529 -0.500000         0.0  \n",
       "46881  0.258819         0.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>ridership</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>datetime</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>lag_1_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21554</th>\n",
       "      <td>2025-01-03</td>\n",
       "      <td>22:00</td>\n",
       "      <td>Abdullah Hukum</td>\n",
       "      <td>Abdullah Hukum</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-01-03 22:00:00</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30734</th>\n",
       "      <td>2025-01-05</td>\n",
       "      <td>14:00</td>\n",
       "      <td>Abdullah Hukum</td>\n",
       "      <td>Abdullah Hukum</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-01-05 14:00:00</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40019</th>\n",
       "      <td>2025-01-06</td>\n",
       "      <td>19:00</td>\n",
       "      <td>Abdullah Hukum</td>\n",
       "      <td>Abdullah Hukum</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-01-06 19:00:00</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45529</th>\n",
       "      <td>2025-01-07</td>\n",
       "      <td>16:00</td>\n",
       "      <td>Abdullah Hukum</td>\n",
       "      <td>Abdullah Hukum</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-01-07 16:00:00</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46881</th>\n",
       "      <td>2025-01-07</td>\n",
       "      <td>19:00</td>\n",
       "      <td>Abdullah Hukum</td>\n",
       "      <td>Abdullah Hukum</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-01-07 19:00:00</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Initializing model with input size: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/ML-Assignment/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /Users/ahanbilbo/Documents/University/University Malaya/Semester 2/Courses/ML/Assignment/ML-Assignment/model/saved_models exists and is not empty.\n",
      "\n",
      "  | Name        | Type       | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | stacks      | ModuleList | 23.8 M | train\n",
      "1 | final_layer | Linear     | 31     | train\n",
      "2 | loss_fn     | MSELoss    | 0      | train\n",
      "---------------------------------------------------\n",
      "23.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.8 M    Total params\n",
      "95.294    Total estimated model params size (MB)\n",
      "363       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting training...\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ML-Assignment/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ML-Assignment/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|‚ñé         | 322/9760 [00:19<09:19, 16.86it/s, train_loss=70.20]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùå Training failed!\n",
      "Error: name 'exit' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x103db1f10>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ML-Assignment/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
